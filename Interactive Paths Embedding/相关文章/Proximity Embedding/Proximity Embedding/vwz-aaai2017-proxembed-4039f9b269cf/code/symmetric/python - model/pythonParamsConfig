[param]

############################################
# training data dictory
############################################
# main work dir
root_dir = D:/test/test/toydata
# the name of one dataset, such as facebook, linkedin
dataset_name = linkedin
# the suffix of dataset, such as 10,100,1000
suffix = 4
# the relation name of data, such as classmate, family, school, work
class_name = work
# the index of the dataset file
index = 3

############################################
# paths for some prepared data
############################################
# the file path of words embeddings
wordsEmbeddings_path = %(root_dir)s/%(dataset_name)s/nodesFeatures
# the file which contains sub-paths
subpaths_file = %(root_dir)s/%(dataset_name)s/subpathsSaveFile

############################################
# experiment parameters - do not need to change frequently
############################################
# the max length for sub-paths
maxlen_subpaths = 1000
# the size of words vocabulary
wordsSize = 1000000
# Sequence longer than this get ignored 
maxlen = 1000
# use a batch for training. This is the size of this batch.
batch_size = 4
# if need shuffle for training
is_shuffle_for_batch = True
# the frequences for display
dispFreq = 2
# the frequences for saving the parameters
saveFreq = 2
# the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index. It will be generated in the code.
saveto = 
# the top num to predict
top_num = 1

############################################
# experiment parameters - need to tune
############################################
# learning rate
lrate = 0.0001
# dimension of words embeddings
word_dimension = 4
# the dimension of paths embeddings
dimension = 5
# the output way of lstm. There are three ways, "h" only uses the last output h as the output of lstm for one path; "mean-pooling" uses the mean-pooling of all hi as the output of lstm for one path; "max-pooling" uses the max-pooling of all hi as the output of lstm for one path.
h_output_method = max-pooling
# the parameter alpha for discount. The longer the subpath, the little will the weight be.
discount_alpha = 0.3
# the ways to combine several subpaths to one. "mean-pooling" means to combine all subpaths to one by mean-pooling; "max-pooling" means to combine all subpaths to one by max-pooling.
subpaths_pooling_method = max-pooling
# loss function, we use sigmoid
objective_function_method = sigmoid
# the parameter in loss function, beta
objective_function_param = 0.5
# the max epochs for training
max_epochs = 50
# decay for lstm_W
decay_lstm_W = 0.0001
# decay for lstm_U
decay_lstm_U = 0.0001
# decay for lstm_b
decay_lstm_b = 0.0001
# decay for w
decay_w = 0.0001
